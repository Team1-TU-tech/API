from kafka import KafkaConsumer
import json
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import boto3
import os, time
from io import BytesIO
from dotenv import load_dotenv
from datetime import datetime, timedelta
import logging
from collections import defaultdict
import threading

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í™˜ê²½ ë³€ìˆ˜ ì½ê¸°
load_dotenv()

KAFKA_SERVER = os.getenv("KAFKA_SERVER")

# Kafka consumer ì„¤ì •
consumer = KafkaConsumer(
    #'logs',
    bootstrap_servers=KAFKA_SERVER,
    group_id='log-consumer-group',
    enable_auto_commit=False,  # ìˆ˜ë™ ì˜¤í”„ì…‹ ì»¤ë°‹ ì„¤ì •
    auto_offset_reset='latest',  # 'earliest' ë˜ëŠ” 'latest' ì„¤ì •
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)
# ì—¬ëŸ¬ í† í”½ì„ êµ¬ë…
consumer.subscribe(['Validate', 'Login_log', 'Logout_log', 'KakaoLogin_log', 'KakaoLogout_log', 'Signup_log', 'View_detail_log' , 'Search_log'])

# S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
s3 = boto3.client('s3',
    aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    region_name="ap-northeast-2"
    )

# ê° í† í”½ì— ëŒ€í•œ ë©”ì‹œì§€ì™€ íƒ€ì´ë¨¸ë¥¼ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
topics_data = {}
countdown_timers = {}

# í† í”½ë³„ ë©”ì‹œì§€ ìˆ˜
topic_message_count = {}

# ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•˜ê³  ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def consume_message(message):
    topic = message.topic
    log_message = message.value

    # ê° í† í”½ë³„ë¡œ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘
    if topic not in topics_data:
        topics_data[topic] = []
        topic_message_count[topic] = 0  # ì¹´ìš´íŠ¸ë¥¼ ì´ˆê¸°í™”

    # ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ìŒ“ëŠ”ë‹¤
    topics_data[topic].append(log_message)
    topic_message_count[topic] += 1  # ë©”ì‹œì§€ ìˆ˜ ì¹´ìš´íŠ¸

    # ì²« ë²ˆì§¸ ë©”ì‹œì§€ì¼ ê²½ìš° ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œì‘
    if topic not in countdown_timers:
        countdown_timers[topic] = threading.Timer(60.0, upload_to_s3, args=[topic])
        countdown_timers[topic].start()

    # ë©”ì‹œì§€ê°€ ì¶”ê°€ë  ë•Œë§ˆë‹¤ íƒ€ì´ë¨¸ë¥¼ ìƒˆë¡œ ì‹œì‘í•´ì„œ 60ì´ˆ í›„ì— ì—…ë¡œë“œ
    countdown_timers[topic].cancel()
    countdown_timers[topic] = threading.Timer(60.0, upload_to_s3, args=[topic])
    countdown_timers[topic].start()

    # ë©”ì‹œì§€ ìˆ˜ê°€ 100ê°œ ì´ìƒì´ë©´ ê° í† í”½ìœ¼ë¡œ S3ì— ì—…ë¡œë“œ
    if topic_message_count[topic] >= 100:
        upload_to_s3(topic)

# S3ì— ì—…ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def upload_to_s3(topic):
    # ì—…ë¡œë“œí•  ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜¤ê¸°
    log_messages = topics_data[topic]

    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.json_normalize(log_messages)

    # DataFrameì„ Parquet í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    table = pa.Table.from_pandas(df)

    # ë©”ëª¨ë¦¬ ë²„í¼ì— Parquet íŒŒì¼ì„ ì €ì¥
    buffer = BytesIO()
    pq.write_table(table, buffer)
    buffer.seek(0)  # ë²„í¼ì˜ ì²˜ìŒìœ¼ë¡œ ì´ë™

    # í˜„ì¬ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    kst_time = datetime.utcnow() + timedelta(hours=9)
    timestamp = kst_time.strftime("%Y-%m-%d_%H-%M")  # ë¶„ í¬í•¨í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±

    # S3 ì—…ë¡œë“œ
    s3.put_object(
        Bucket='t1-tu-data',
        Key=f'logs/{topic}/{timestamp}.parquet',  # ê° í† í”½ë³„ë¡œ ê²½ë¡œë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •
        Body=buffer
    )

    # ì—…ë¡œë“œ í›„ ì´ˆê¸°í™”
    logger = logging.getLogger()
    logger.info(f'ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ‰ ë¡œê·¸ê°€ S3ì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: logs/{topic}/{timestamp}.parquet ğŸ‰ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸ¢')

    # ì—…ë¡œë“œí•œ ë©”ì‹œì§€ì™€ ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
    topics_data[topic] = []  # ë©”ì‹œì§€ ì´ˆê¸°í™”
    topic_message_count[topic] = 0  # ë©”ì‹œì§€ ìˆ˜ ì´ˆê¸°í™”
    countdown_timers[topic].cancel()  # íƒ€ì´ë¨¸ ì´ˆê¸°í™”

# ë©”ì‹œì§€ ìˆ˜ì‹  ë° ì²˜ë¦¬ ì‹œì‘
for message in consumer:
    consume_message(message)

if __name__ == '__main__':

